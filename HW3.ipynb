{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "18e64aa0",
   "metadata": {},
   "source": [
    "# HW3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d42027d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa16be7d",
   "metadata": {},
   "source": [
    "## Task 1: Framework"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32dde7b1",
   "metadata": {},
   "source": [
    "PyTorch is the frame work I will be using for this project. It is chosen because of it's simplicity (declarative layers) while also giving the developer very low level control of the underlying data (gradients, bias, etc...). Also PyTorch offers a OOP model approach for developers to package individual models as a class and re-use them in the future. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf62cab1",
   "metadata": {},
   "source": [
    "Here are some important modules to implement this 2 layer Neural Network\n",
    "\n",
    "- **nn.Linear**\n",
    "    - Defines a fully connected layer, which is the most basic component of a neural network.\n",
    "    - https://pytorch.org/docs/stable/generated/torch.nn.Linear.html?highlight=nn%20linear#torch.nn.Linear\n",
    "- **nn.Module**\n",
    "    - Base class of all neural networks for the PyTorch framework.\n",
    "    - https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module\n",
    "- **optim.Adam**\n",
    "    - Adam optimizer, which is a modified gradient descent with variable learning rate.\n",
    "    - https://pytorch.org/docs/stable/generated/torch.optim.Adam.html?highlight=adam#torch.optim.Adam\n",
    "- **functional.cross_entropy**\n",
    "    - Cross entropy loss function. Used for calculating the loss of the classification problem, and also serves as the entry point for calculating gradient and doing back propagation.\n",
    "    - https://pytorch.org/docs/stable/generated/torch.nn.functional.cross_entropy.html?highlight=cross_entropy#torch.nn.functional.cross_entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e60ace33",
   "metadata": {},
   "source": [
    "## Task 2: Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da737a29",
   "metadata": {},
   "source": [
    "**Dataset**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73c2b37d",
   "metadata": {},
   "source": [
    "The dataset of choice is CIFAR-10, which consists of 60000 32x32 colour images in 10 classes, with 6000 images per class. The 10 labels are \n",
    "1. airplane\t\t\t\t\t\t\t\t\t\t\n",
    "1. automobile\t\t\t\t\t\t\t\t\t\t\n",
    "1. bird\t\t\t\t\t\t\t\t\t\t\n",
    "1. cat\t\t\t\t\t\t\t\t\t\t\n",
    "1. deer\t\t\t\t\t\t\t\t\t\t\n",
    "1. dog\t\t\t\t\t\t\t\t\t\t\n",
    "1. frog\t\t\t\t\t\t\t\t\t\t\n",
    "1. horse\t\t\t\t\t\t\t\t\t\t\n",
    "1. ship\t\t\t\t\t\t\t\t\t\t\n",
    "1. truck"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dab5c34",
   "metadata": {},
   "source": [
    "**Load dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "301de556",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CIFAR_10(Dataset):\n",
    "    '''\n",
    "    CIFAR 10 dataset\n",
    "    '''\n",
    "    def __init__(self, filenames):\n",
    "        self.filenames = filenames\n",
    "        # read in dataset\n",
    "        self.data = []\n",
    "        self.labels = []\n",
    "        for filename in self.filenames:\n",
    "            raw = self.unpickle(filename)\n",
    "            self.data.append(raw[b'data'])\n",
    "            self.labels.append(raw[b'labels'])\n",
    "        # concat into torch tensors\n",
    "        self.data = torch.cat([torch.tensor(x, dtype=torch.float64) for x in self.data])\n",
    "        self.labels = torch.cat([torch.tensor(x, dtype=torch.uint8) for x in self.labels])\n",
    "        # normalize dataset\n",
    "        self.data /= 255\n",
    "    \n",
    "    def unpickle(self, file):\n",
    "        with open(file, 'rb') as fo:\n",
    "            data = pickle.load(fo, encoding='bytes')\n",
    "        return data\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.labels.shape[0]\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx], self.labels[idx]\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5649a40a",
   "metadata": {},
   "source": [
    "Load train, valid, and test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "be36c3d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = CIFAR_10([f'../data/CIFAR_10/cifar-10-batches-py/data_batch_{i}' for i in range(1, 5)])\n",
    "valid_dataset = CIFAR_10(['../data/CIFAR_10/cifar-10-batches-py/data_batch_5'])\n",
    "test_dataset = CIFAR_10(['../data/CIFAR_10/cifar-10-batches-py/test_batch'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "896061e9",
   "metadata": {},
   "source": [
    "Load label names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "ba4f3daf",
   "metadata": {},
   "outputs": [],
   "source": [
    "meta = None\n",
    "with open('../data/CIFAR_10/cifar-10-batches-py/batches.meta', 'rb') as fo:\n",
    "    meta = pickle.load(fo, encoding='bytes')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "109631ee",
   "metadata": {},
   "source": [
    "**Sample data visualization**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "dcdc7e6b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x15525c93370>"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "img, lbl = train_dataset.__getitem__(1)\n",
    "img = torch.moveaxis((img.reshape((3, 32, 32)) * 255).long(), 0, -1)\n",
    "plt.title(meta[b'label_names'][lbl])\n",
    "plt.imshow(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ef6fa8e",
   "metadata": {},
   "source": [
    "**Train/Dev/Test Split**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69120179",
   "metadata": {},
   "source": [
    "The CIFAR-10 dataset was pre-splitted into 5 training batches and 1 testing batches, with each batches containing 10000 entries. For this project I will use the one of the 5 training batches as the dev/validation dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72d53d76",
   "metadata": {},
   "source": [
    "**Final cost function**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db14536f",
   "metadata": {},
   "source": [
    "Since it is a multiple label classification problem, cross entropy loss is used. The resulting value from the neural network is first passed into a softmax function before being passed into cross entropy. Final cost looks like below:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58800b57",
   "metadata": {},
   "source": [
    "$$\n",
    "CE_{loss} = - \\sum^C_i t_i \\log(\\frac{e^{s_i}}{\\sum^C_j e^{s_j}})\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4073d713",
   "metadata": {},
   "source": [
    "**Model implementation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "5778ead8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, h1_size = 128, h2_size = 64):\n",
    "        '''\n",
    "        Simple DNN with 2 layers of CNN followed by 3 linear layers\n",
    "        '''\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 6, 4)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(6, 12, 4)\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.f1 = nn.Linear(12 * 11 * 11, h1_size)\n",
    "        self.f2 = nn.Linear(h1_size, h2_size)\n",
    "        self.drop1 = nn.Dropout()\n",
    "        self.f3 = nn.Linear(h2_size, 10)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.pool(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.f1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.f2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.drop1(x)\n",
    "        x = self.f3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c82474ca",
   "metadata": {},
   "source": [
    "Hyper parameter configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "4507feb2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLP(\n",
       "  (conv1): Conv2d(3, 6, kernel_size=(4, 4), stride=(1, 1))\n",
       "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (conv2): Conv2d(6, 12, kernel_size=(4, 4), stride=(1, 1))\n",
       "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
       "  (f1): Linear(in_features=1452, out_features=128, bias=True)\n",
       "  (f2): Linear(in_features=128, out_features=64, bias=True)\n",
       "  (drop1): Dropout(p=0.5, inplace=False)\n",
       "  (f3): Linear(in_features=64, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = f'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "batch_size = 512\n",
    "epochs = 30\n",
    "learning_rate = 0.001\n",
    "\n",
    "model = MLP(h1_size = 128, h2_size = 64)\n",
    "model.to(device)\n",
    "model.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fe7a331",
   "metadata": {},
   "source": [
    "Spawn dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "29371afa",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(dataset = train_dataset, batch_size = batch_size)\n",
    "valid_loader = DataLoader(dataset = valid_dataset, batch_size = batch_size)\n",
    "test_loader = DataLoader(dataset = test_dataset, batch_size = batch_size)\n",
    "optimizer = optim.Adam(model.parameters(), lr = learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9ec049a",
   "metadata": {},
   "source": [
    "**Training**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "86a6d654",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████| 79/79 [00:01<00:00, 74.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, train loss: 0.32328672893345356\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████| 79/79 [00:01<00:00, 78.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, train loss: 0.2848415777552873\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████| 79/79 [00:01<00:00, 77.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, train loss: 0.26577356993220747\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████| 79/79 [00:01<00:00, 70.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3, train loss: 0.25555909634567797\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████| 79/79 [00:01<00:00, 77.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4, train loss: 0.2479245278518647\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████| 79/79 [00:01<00:00, 77.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5, train loss: 0.24072468606755137\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████| 79/79 [00:01<00:00, 75.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6, train loss: 0.23492203536443412\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████| 79/79 [00:01<00:00, 72.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7, train loss: 0.22935114218853414\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████| 79/79 [00:01<00:00, 67.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8, train loss: 0.2243683929555118\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████| 79/79 [00:01<00:00, 72.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9, train loss: 0.21925282129086554\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████| 79/79 [00:01<00:00, 74.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10, train loss: 0.21452953317202628\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████| 79/79 [00:01<00:00, 75.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11, train loss: 0.20950802671723068\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████| 79/79 [00:01<00:00, 76.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12, train loss: 0.20513562485575676\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████| 79/79 [00:01<00:00, 75.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13, train loss: 0.2023393149720505\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████| 79/79 [00:01<00:00, 76.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14, train loss: 0.19927469512913376\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████| 79/79 [00:01<00:00, 70.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15, train loss: 0.19459954975172877\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████| 79/79 [00:01<00:00, 77.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16, train loss: 0.19172520632855594\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████| 79/79 [00:01<00:00, 77.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17, train loss: 0.1872442023595795\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████| 79/79 [00:01<00:00, 77.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18, train loss: 0.18510092434007674\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████| 79/79 [00:01<00:00, 77.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19, train loss: 0.1818123406264931\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████| 79/79 [00:01<00:00, 70.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20, train loss: 0.17789803782943636\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████| 79/79 [00:01<00:00, 76.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21, train loss: 0.17464986315462738\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████| 79/79 [00:01<00:00, 77.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22, train loss: 0.17188518366310745\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████| 79/79 [00:01<00:00, 77.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23, train loss: 0.17025469965301454\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████| 79/79 [00:01<00:00, 74.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24, train loss: 0.16870393813587725\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████| 79/79 [00:01<00:00, 77.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25, train loss: 0.1672387425787747\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████| 79/79 [00:01<00:00, 77.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26, train loss: 0.16577990393852815\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████| 79/79 [00:01<00:00, 77.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27, train loss: 0.16151188872754574\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████| 79/79 [00:01<00:00, 71.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28, train loss: 0.15941607189597562\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████| 79/79 [00:01<00:00, 77.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29, train loss: 0.15531852294225246\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    sum_loss = 0\n",
    "    for (data, labels) in tqdm(train_loader):\n",
    "        \n",
    "        # zero out gradient\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # send batch to device\n",
    "        data, labels = data.to(device), labels.to(device)\n",
    "       \n",
    "        # foward pass\n",
    "        pred = model(data.view(labels.shape[0], 3, 32, 32).float())\n",
    "        \n",
    "        # cross entropy loss\n",
    "        loss = F.cross_entropy(pred, labels)\n",
    "        \n",
    "        # back propagation\n",
    "        loss.backward()\n",
    "        sum_loss += loss.item()\n",
    "        optimizer.step()\n",
    "    \n",
    "    # print out epoch stats\n",
    "    print(f'Epoch {epoch}, train loss: {sum_loss / batch_size}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e92941b",
   "metadata": {},
   "source": [
    "**Evaluation**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8868696b",
   "metadata": {},
   "source": [
    "Training and Validation Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "4e28ae46",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████| 79/79 [00:00<00:00, 115.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy: 0.697\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    for (data, labels) in tqdm(train_loader):\n",
    "\n",
    "        # send batch to device\n",
    "        data, labels = data.to(device), labels.to(device)\n",
    "\n",
    "        # foward pass\n",
    "        pred = model(data.view(labels.shape[0], 3, 32, 32).float())\n",
    "\n",
    "        # softmax and argmax\n",
    "        pred = F.softmax(pred, dim = 1)\n",
    "        pred = torch.argmax(pred, dim = 1)\n",
    "\n",
    "        # compute accuracy\n",
    "        correct += torch.sum(pred == labels.float())\n",
    "\n",
    "# accuracy\n",
    "print(f'Training Accuracy: {correct / len(train_dataset):.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "b2167476",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████| 20/20 [00:00<00:00, 119.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.585\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    for (data, labels) in tqdm(valid_loader):\n",
    "\n",
    "        # send batch to device\n",
    "        data, labels = data.to(device), labels.to(device)\n",
    "\n",
    "        # foward pass\n",
    "        pred = model(data.view(labels.shape[0], 3, 32, 32).float())\n",
    "\n",
    "        # softmax and argmax\n",
    "        pred = F.softmax(pred, dim = 1)\n",
    "        pred = torch.argmax(pred, dim = 1)\n",
    "\n",
    "        # compute accuracy\n",
    "        correct += torch.sum(pred == labels.float())\n",
    "\n",
    "# accuracy\n",
    "print(f'Validation Accuracy: {correct / len(valid_dataset):.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59e6039e",
   "metadata": {},
   "source": [
    "Testing Accuracy (Only ran after tuning hyper-parameters using validation dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "7532cfab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████| 20/20 [00:00<00:00, 111.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Accuracy: 0.583\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    for (data, labels) in tqdm(test_loader):\n",
    "\n",
    "        # send batch to device\n",
    "        data, labels = data.to(device), labels.to(device)\n",
    "\n",
    "        # foward pass\n",
    "        pred = model(data.view(labels.shape[0], 3, 32, 32).float())\n",
    "\n",
    "        # softmax and argmax\n",
    "        pred = F.softmax(pred, dim = 1)\n",
    "        pred = torch.argmax(pred, dim = 1)\n",
    "\n",
    "        # compute accuracy\n",
    "        correct += torch.sum(pred == labels.float())\n",
    "\n",
    "# accuracy\n",
    "print(f'Testing Accuracy: {correct / len(test_dataset):.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4b2d3f0",
   "metadata": {},
   "source": [
    "## Task 3: Hyper-parameter tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8695cfce",
   "metadata": {},
   "source": [
    "The hyper-parameters I'm intersted in tuning are the following:\n",
    "1. Number of epochs\n",
    "1. Batch size\n",
    "1. Learning rate\n",
    "1. Hidden layer 1 size\n",
    "1. Hidden layer 2 size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "512d70ba",
   "metadata": {},
   "source": [
    "The technique I used is the \"grid search\" method. Basically, For each hyper-parameters above I have a set range of possible values. I then train a neural networks for every possible combinations of the hyper-parameters above, and pick the model with the best validation accuracy. The idea is that given a carefully selected range of reasonable values, I can brute-force out the best combination of the above hyper-parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ace122a",
   "metadata": {},
   "source": [
    "For regularization I implemented a dropout layer with probability 0.5. This is because I start to notice some degree of overfitting (training loss is going down while validation loss isn't) when training the model to higher epochs. Using this technique significantly generalized the model and also boosted my validation accuracy. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16845dee",
   "metadata": {},
   "source": [
    "For optimization, I used the Adam optimizer. This is to prevent the gradient being stuck at a local minima by changing the learning rate using exponential moving accuracy. Generally, using Adam optimzer allows the model to converge faster."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f55cce9b",
   "metadata": {},
   "source": [
    "##  Task 4: Non-neural-network model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "f102143a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bc53273",
   "metadata": {},
   "source": [
    "**Train a Decision Tree**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "cd6aad49",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier()"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = DecisionTreeClassifier()\n",
    "clf.fit(train_dataset.data, train_dataset.labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afa3d7af",
   "metadata": {},
   "source": [
    "Testing accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "c310f614",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Accuracy: 0.264\n"
     ]
    }
   ],
   "source": [
    "print(f'Testing Accuracy: {clf.score(test_dataset.data, test_dataset.labels):.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d568e3ae",
   "metadata": {},
   "source": [
    "**Comparison**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc9c0017",
   "metadata": {},
   "source": [
    "The DNN we trained has a test accuracy of 0.583, while the decision tree has less than half of that (0.264). While both model performs better than random guesses (0.1 accuracy), there exists a huge gap between the DNN and the Decision Tree. Here are some reasons to explain the difference in performance between the two model:\n",
    "\n",
    "- The curse of dimensionality: There are simply way too many dimensions (3072 features) for the tree to make any meaningful cuts, since on high dimensionality, a lot of unrelated data looks similar. On the other hand, the DNN we trained compresses the data with 2 layers of CNN, before being passed into 2 more hidden layers to compute the decision, which allows for efficient feature extraction. \n",
    "\n",
    "- Overfitting: The decision tree is most definitely overfitting the training data, which impact its performance when evaluating on a dataset that it has not seen before. \n",
    "\n",
    "- Learning capability: Decision tree is more of an art than science. Unlike a neural network, it does not have the ability to learn and extract features such as a wing on a plane or a wheel on a truck. It simply computes the best dimension to cut that results in the highest information gain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f729a3a8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
